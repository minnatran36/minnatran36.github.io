{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e76f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:31:07.896620Z",
     "iopub.status.busy": "2026-01-22T01:31:07.895676Z",
     "iopub.status.idle": "2026-01-22T01:31:07.905926Z",
     "shell.execute_reply": "2026-01-22T01:31:07.904324Z"
    },
    "papermill": {
     "duration": 0.016233,
     "end_time": "2026-01-22T01:31:07.908076",
     "exception": false,
     "start_time": "2026-01-22T01:31:07.891843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instacart-online-grocery-basket-analysis-dataset']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/kaggle/input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d8b116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T01:31:07.913807Z",
     "iopub.status.busy": "2026-01-22T01:31:07.913471Z",
     "iopub.status.idle": "2026-01-22T02:17:39.314047Z",
     "shell.execute_reply": "2026-01-22T02:17:39.312839Z"
    },
    "papermill": {
     "duration": 2791.406657,
     "end_time": "2026-01-22T02:17:39.316477",
     "exception": false,
     "start_time": "2026-01-22T01:31:07.909820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRODUCT2VEC TRAINING PIPELINE\n",
      "============================================================\n",
      "\n",
      "[STEP 1] Loading CSV files...\n",
      "  ✓ Loaded 32,434,489 order-product rows\n",
      "  ✓ Loaded 49,688 products\n",
      "\n",
      "[STEP 2] Joining tables...\n",
      "  ✓ Joined table: 32,434,489 rows\n",
      "\n",
      "[STEP 3] Creating baskets...\n",
      "  ✓ Created 3,214,874 baskets in 61.2s\n",
      "  ✓ After filtering: 3,058,106 baskets\n",
      "\n",
      "Sample baskets:\n",
      "  1: ['Organic Egg Whites', 'Michigan Organic Kale', 'Garlic Powder', 'Coconut Butter', '...']\n",
      "  2: ['Total 2% with Strawberry Lowfat Greek Strained Yogurt', 'Unsweetened Almondmilk', 'Lemons', 'Organic Baby Spinach', '...']\n",
      "  3: ['Plain Pre-Sliced Bagels', 'Honey/Lemon Cough Drops', 'Chewy 25% Low Sugar Chocolate Chip Granola', 'Oats & Chocolate Chewy Bars', '...']\n",
      "\n",
      "[STEP 4] Training Word2Vec...\n",
      "  Config: vector_size=100, window=10, min_count=5, epochs=10\n",
      "  This takes ~10-15 minutes...\n",
      "\n",
      "  ✓ Training complete in 2312.9s (38.5 min)\n",
      "  ✓ Vocabulary: 47,575 products\n",
      "\n",
      "[STEP 5] Testing model...\n",
      "\n",
      "Similar to 'Banana':\n",
      "  • Organic Fuji Apple: 0.755\n",
      "  • Unsweetened Original Almond Breeze Almond Milk: 0.738\n",
      "  • Vanilla Almond Breeze Almond Milk: 0.704\n",
      "  • Seedless Red Grapes: 0.679\n",
      "  • Granny Smith Apples: 0.666\n",
      "\n",
      "[STEP 6] Exporting for web...\n",
      "  ✓ Prepared 47,575 products\n",
      "  Computing similarities (this takes a few minutes)...\n",
      "    Processing 0/47,575...\n",
      "    Processing 5,000/47,575...\n",
      "    Processing 10,000/47,575...\n",
      "    Processing 15,000/47,575...\n",
      "    Processing 20,000/47,575...\n",
      "    Processing 25,000/47,575...\n",
      "    Processing 30,000/47,575...\n",
      "    Processing 35,000/47,575...\n",
      "    Processing 40,000/47,575...\n",
      "    Processing 45,000/47,575...\n",
      "  ✓ Computed similarities for 47,575 products\n",
      "\n",
      "[STEP 7] Saving files...\n",
      "  ✓ Saved products.json\n",
      "  ✓ Saved similarities.json\n",
      "  ✓ Saved config.json\n",
      "\n",
      "============================================================\n",
      "COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Download these files from the 'Output' tab on the right:\n",
      "  • products.json\n",
      "  • similarities.json\n",
      "  • config.json\n",
      "\n",
      "Then put them in your website's web_data/ folder!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PRODUCT2VEC - COMPLETE KAGGLE NOTEBOOK\n",
    "# ============================================================================\n",
    "# This runs everything: data processing → training → export\n",
    "# Takes about 15-20 minutes total\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRODUCT2VEC TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD AND PROCESS DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading CSV files...\")\n",
    "\n",
    "# Kaggle input path\n",
    "INPUT_PATH = \"/kaggle/input/instacart-online-grocery-basket-analysis-dataset\"\n",
    "\n",
    "# Load order_products__prior.csv (32M rows)\n",
    "order_products = pd.read_csv(\n",
    "    f\"{INPUT_PATH}/order_products__prior.csv\",\n",
    "    usecols=[\"order_id\", \"product_id\"]\n",
    ")\n",
    "print(f\"  ✓ Loaded {len(order_products):,} order-product rows\")\n",
    "\n",
    "# Load products.csv\n",
    "products = pd.read_csv(\n",
    "    f\"{INPUT_PATH}/products.csv\",\n",
    "    usecols=[\"product_id\", \"product_name\"]\n",
    ")\n",
    "print(f\"  ✓ Loaded {len(products):,} products\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: JOIN TABLES\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Joining tables...\")\n",
    "\n",
    "joined = pd.merge(order_products, products, on=\"product_id\", how=\"left\")\n",
    "print(f\"  ✓ Joined table: {len(joined):,} rows\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CREATE BASKETS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Creating baskets...\")\n",
    "\n",
    "start_time = time.time()\n",
    "baskets = joined.groupby(\"order_id\")[\"product_name\"].apply(list).tolist()\n",
    "print(f\"  ✓ Created {len(baskets):,} baskets in {time.time()-start_time:.1f}s\")\n",
    "\n",
    "# Filter baskets (keep size 2-100)\n",
    "baskets = [b for b in baskets if 2 <= len(b) <= 100]\n",
    "print(f\"  ✓ After filtering: {len(baskets):,} baskets\")\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\nSample baskets:\")\n",
    "for i, b in enumerate(baskets[:3]):\n",
    "    display = b[:4] + [\"...\"] if len(b) > 4 else b\n",
    "    print(f\"  {i+1}: {display}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: TRAIN WORD2VEC\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Training Word2Vec...\")\n",
    "print(\"  Config: vector_size=100, window=10, min_count=5, epochs=10\")\n",
    "print(\"  This takes ~10-15 minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=baskets,\n",
    "    vector_size=100,\n",
    "    window=10,\n",
    "    min_count=5,\n",
    "    negative=10,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=10,\n",
    "    workers=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"  ✓ Training complete in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"  ✓ Vocabulary: {len(model.wv):,} products\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TEST THE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Testing model...\")\n",
    "\n",
    "test_products = [\"Banana\", \"Organic Whole Milk\", \"Bag of Organic Bananas\"]\n",
    "for product in test_products:\n",
    "    if product in model.wv:\n",
    "        print(f\"\\nSimilar to '{product}':\")\n",
    "        for p, score in model.wv.most_similar(product, topn=5):\n",
    "            print(f\"  • {p}: {score:.3f}\")\n",
    "        break\n",
    "else:\n",
    "    # Find any product to test\n",
    "    test = list(model.wv.index_to_key)[0]\n",
    "    print(f\"\\nSimilar to '{test}':\")\n",
    "    for p, score in model.wv.most_similar(test, topn=5):\n",
    "        print(f\"  • {p}: {score:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: EXPORT FOR WEB\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 6] Exporting for web...\")\n",
    "\n",
    "# Export vectors\n",
    "vectors_dict = {product: model.wv[product].tolist() for product in model.wv.index_to_key}\n",
    "\n",
    "# Export product list with categories\n",
    "products_full = pd.read_csv(f\"{INPUT_PATH}/products.csv\")\n",
    "aisles = pd.read_csv(f\"{INPUT_PATH}/aisles.csv\")\n",
    "departments = pd.read_csv(f\"{INPUT_PATH}/departments.csv\")\n",
    "\n",
    "products_full = products_full.merge(aisles, on=\"aisle_id\", how=\"left\")\n",
    "products_full = products_full.merge(departments, on=\"department_id\", how=\"left\")\n",
    "\n",
    "products_list = []\n",
    "for product_name in model.wv.index_to_key:\n",
    "    row = products_full[products_full[\"product_name\"] == product_name]\n",
    "    if len(row) > 0:\n",
    "        products_list.append({\n",
    "            \"name\": product_name,\n",
    "            \"aisle\": row.iloc[0].get(\"aisle\", \"unknown\"),\n",
    "            \"department\": row.iloc[0].get(\"department\", \"unknown\")\n",
    "        })\n",
    "    else:\n",
    "        products_list.append({\n",
    "            \"name\": product_name,\n",
    "            \"aisle\": \"unknown\",\n",
    "            \"department\": \"unknown\"\n",
    "        })\n",
    "\n",
    "print(f\"  ✓ Prepared {len(products_list):,} products\")\n",
    "\n",
    "# Compute similarities (top 20 for each product)\n",
    "print(\"  Computing similarities (this takes a few minutes)...\")\n",
    "\n",
    "product_names = list(model.wv.index_to_key)\n",
    "similarities = {}\n",
    "\n",
    "for i, product in enumerate(product_names):\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"    Processing {i:,}/{len(product_names):,}...\")\n",
    "    similar = model.wv.most_similar(product, topn=20)\n",
    "    similarities[product] = [[name, round(score, 4)] for name, score in similar]\n",
    "\n",
    "print(f\"  ✓ Computed similarities for {len(similarities):,} products\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: SAVE FILES\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 7] Saving files...\")\n",
    "\n",
    "# Save to Kaggle output\n",
    "with open(\"/kaggle/working/products.json\", \"w\") as f:\n",
    "    json.dump(products_list, f)\n",
    "print(\"  ✓ Saved products.json\")\n",
    "\n",
    "with open(\"/kaggle/working/similarities.json\", \"w\") as f:\n",
    "    json.dump(similarities, f)\n",
    "print(\"  ✓ Saved similarities.json\")\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"Product2Vec\",\n",
    "    \"num_products\": len(model.wv),\n",
    "    \"vector_dimensions\": 100,\n",
    "    \"training_baskets\": len(baskets),\n",
    "    \"algorithm\": \"Skip-gram with Negative Sampling\"\n",
    "}\n",
    "with open(\"/kaggle/working/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"  ✓ Saved config.json\")\n",
    "\n",
    "# ============================================================================\n",
    "# DONE!\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Download these files from the 'Output' tab on the right:\n",
    "  • products.json\n",
    "  • similarities.json\n",
    "  • config.json\n",
    "\n",
    "Then put them in your website's web_data/ folder!\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1889830,
     "sourceId": 3092582,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2797.938427,
   "end_time": "2026-01-22T02:17:42.043104",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T01:31:04.104677",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
